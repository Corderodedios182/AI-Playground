{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51ac7339-6484-482c-b69c-dc1a2fb94779",
   "metadata": {},
   "source": [
    "**Chatbot con Generación Aumentada de Recuperación**\n",
    "\n",
    "En esta sección, demostramos algunos casos de uso comunes de incrustación con algunos ejemplos simples. La distancia euclidiana se utiliza para calcular la similitud entre dos fragmentos de texto.\n",
    "\n",
    "Búsqueda y recomendación\n",
    "Supongamos que tiene una colección de documentos (el conjunto de datos). Cada documento está representado por su incrustación. Se le ha proporcionado una cadena de consulta. La solicitud es identificar el documento que es más relevante para la cadena de consulta. Puede lograr esto con los siguientes pasos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42d782a7-9e32-4308-9c11-b14c64036871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albert Einstein fue un físico y matemático que nació el 14 de marzo de 1879 en Ulm (Alemania). Es conocido sobre todo por sus contribuciones a la mecánica cuántica, la física relativista y sobre todo por el Teorema de Einstein de Relatividad.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "bedrock = boto3.client(\n",
    "    service_name='bedrock-runtime'\n",
    ")\n",
    "modelId = 'ai21.j2-ultra'\n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'\n",
    "prompt = \"\"\"¿Quien era Albert Eistein?\"\"\"\n",
    "\n",
    "input = {'prompt': prompt, 'maxTokens': 200}\n",
    "body=json.dumps(input)\n",
    "response = bedrock.invoke_model(body=body, modelId=modelId, accept=accept,contentType=contentType)\n",
    "response_body = json.loads(response.get('body').read())\n",
    "completions = response_body['completions']\n",
    "for part in completions:\n",
    "    print(part['data']['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541f7925-e5ff-4fcf-a2f7-1c50c6519b93",
   "metadata": {},
   "source": [
    "Suponiendo que conozcas a otro Ablert Einstein. \n",
    "\n",
    "Este Einstein tiene el mismo nombre que el mundialmente famoso Einstein, pero vende conchas marinas en todo el mundo. Piense en el hecho de que los modelos de fundación se entrenan con información o conocimiento de dominio público. Este Einstein, aunque su negocio de conchas marinas puede ser bastante exitoso, pero hay muy poca información o conocimiento sobre él y su negocio en el dominio público. Por esta razón, es poco probable que el modelo de fundación sea consciente de este Einstein y genere respuestas relacionadas con este Einstein, en comparación con ese Einstein.\n",
    "\n",
    "Sin embargo, si proporcionamos alguna información o conocimiento adicional sobre este Einstein en el prompt, el modelo básico entonces sabe que estamos preguntando sobre este Einstein, no sobre ese Einstein. La información o el conocimiento adicional en el mensaje generalmente se conoce como el contexto de la conversación.\n",
    "\n",
    "A continuación se muestra un mensaje con contexto adicional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c081e97-4a8c-4c61-b3a3-6d78e2d94655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "No rinde ninguna de las respuestas correctas\n",
      "Albert Einstein se dedico a la física y el estudio de la relatividad general.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Utiliza el siguiente contexto para responder a la pregunta:\n",
    "\n",
    "=== Contexto ===\n",
    "Albert Einstein vende conchas marinas en Los Ángeles.\n",
    "Albert Einstein regenta una tienda en Los Ángeles.\n",
    "Albert Einstein vende conchas marinas en Sydney.\n",
    "Albert Einstein vende conchas marinas en Seúl.\n",
    "Albert Einstein vende conchas marinas en Pekín.\n",
    "\n",
    "=== Pregunta ===\n",
    "¿A qué se dedica Albert Einstein?\n",
    "            \"\"\"\n",
    "\n",
    "input = {'prompt': prompt, 'maxTokens': 200}\n",
    "body=json.dumps(input)\n",
    "response = bedrock.invoke_model(body=body, modelId=modelId, accept=accept,contentType=contentType)\n",
    "response_body = json.loads(response.get('body').read())\n",
    "completions = response_body['completions']\n",
    "for part in completions:\n",
    "    print(part['data']['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafaca35-4ad6-40c6-beea-6769ae3568e2",
   "metadata": {},
   "source": [
    "Ahora te preguntarás, si el usuario que hace la pregunta ya sabe que Albert Einstein vende conchas marinas en todo el mundo, ¿por qué le preguntaría a un chatbot? En realidad, hacemos una pregunta cuando no sabemos la respuesta, no cuando sabemos la respuesta.\n",
    "\n",
    "Suponiendo que tengamos alguna información o conocimiento sobre este Albert Einstein y los pongamos a disposición del chatbot. El usuario no tiene acceso a dicha información o conocimiento, pero puede preguntar al chatbot. ¿Cómo puede el chatbot aprovechar la información/conocimiento adicional para proporcionar respuestas sensibles al contexto al usuario?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8920de79-9caf-445c-a3f6-69d3d423de71",
   "metadata": {},
   "source": [
    "**Generación aumentada de recuperación (RAG)**\n",
    "\n",
    "Supongamos que tenemos una base de conocimientos con alguna información o conocimiento que no está disponible en el dominio público. Con la generación aumentada de recuperación, utilizamos los siguientes pasos para generar respuestas sensibles al contexto:\n",
    "\n",
    "Convierta el mensaje (texto de la pregunta) en incrustación.\n",
    "\n",
    "- (R) Recupere N entradas más relevantes de la base de conocimientos. Esto se trata como el contexto de la conversación.\n",
    "- (A) Aumente la indicación con el contexto anteponiendo el contexto al texto de la pregunta. Este resultado final es el mensaje contextual.\n",
    "- (G) Genere la respuesta alimentando el mensaje sensible al contexto al modelo básico.\n",
    "\n",
    "En este ejercicio, usamos la colección OpenSearch Serverless como base de conocimiento. Tenga en cuenta que en el índice de demostración ya tenemos algunos datos falsos de **dataset.json**\n",
    "\n",
    "En el código de ejemplo siguiente se muestra cómo realizar la generación aumentada de recuperación. El texto de la consulta es \"¿Qué hace Albert Einstein?\" y queremos las 5 entradas más relevantes de la base de conocimiento como contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "704d1595-aeca-4ac4-bcec-571de45c6cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "He is a support engineer\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "from requests_aws4auth import AWS4Auth\n",
    "\n",
    "region = 'us-west-2'\n",
    "host = 'https://5tl3o6l27r1pcm2hddk9.us-west-2.aoss.amazonaws.com'\n",
    "\n",
    "def get_embedding(bedrock, text):\n",
    "    modelId = 'amazon.titan-embed-text-v1'\n",
    "    accept = 'application/json'\n",
    "    contentType = 'application/json'\n",
    "    input = {\n",
    "            'inputText': text\n",
    "        }\n",
    "    body=json.dumps(input)\n",
    "    response = bedrock.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept,contentType=contentType)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    embedding = response_body['embedding']\n",
    "    return embedding\n",
    "    \n",
    "def search(embedding, limit=1):\n",
    "    # prepare for OpenSearch Serverless\n",
    "    service = 'aoss'\n",
    "    credentials = boto3.Session().get_credentials()\n",
    "    awsauth = AWS4Auth(\n",
    "        credentials.access_key, \n",
    "        credentials.secret_key, \n",
    "        \"us-west-2\", \n",
    "        service, \n",
    "        session_token=credentials.token\n",
    "    )\n",
    "    # search\n",
    "    index = 'demo-index'\n",
    "    datatype = '_search'\n",
    "    url = host + '/' + index + '/' + datatype\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    document = {\n",
    "        'size': limit,\n",
    "        'query': {\n",
    "            'knn': {\n",
    "                'embedding': {\n",
    "                    'vector': embedding,\n",
    "                    'k': limit\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    # response\n",
    "    response = requests.get(url, auth=awsauth, json=document, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    output = ''\n",
    "    for item in data['hits']['hits']:\n",
    "        output += item['_source']['content'] + '\\n'\n",
    "    return output\n",
    "\n",
    "# main function\n",
    "bedrock = boto3.client(\n",
    "    service_name='bedrock-runtime'\n",
    ")\n",
    "# this is the original prompt (query text)\n",
    "prompt = 'What does Albert Einstein do?'\n",
    "# convet the query text into embedding\n",
    "embedding = get_embedding(bedrock, prompt)\n",
    "# retrieve 5 most relevant entries from the knowledge base\n",
    "info = search(embedding, limit=5)\n",
    "# augment the prompt with the context\n",
    "prompt = 'Use the context below to answer the question:\\n\\n=== Context ===\\n{0}\\n\\n=== Question ===\\n{1}'.format(info, prompt)\n",
    "# ask the foundation model\n",
    "modelId = 'ai21.j2-ultra'\n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'\n",
    "input = {'prompt': prompt, 'maxTokens': 200}\n",
    "body=json.dumps(input)\n",
    "response = bedrock.invoke_model(body=body, modelId=modelId, accept=accept,contentType=contentType)\n",
    "response_body = json.loads(response.get('body').read())\n",
    "completions = response_body['completions']\n",
    "for part in completions:\n",
    "    print(part['data']['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3d795b-a8d4-442d-9616-3d8ae2ce87b5",
   "metadata": {},
   "source": [
    "**Integración de chatbot**\n",
    "\n",
    "Ahora modificaremos el código de nuestra función Lambda para agregar la capacidad de realizar la generación aumentada de recuperación. Si el mensaje humano comienza con, se trata como una solicitud de generación aumentada de recuperación. El chatbot extrae el contenido después del comando como texto de consulta, busca N entradas más relevantes de la base de conocimiento, utiliza el conocimiento como contexto para interactuar con el modelo base, devuelve la respuesta del modelo base al usuario final.//rag\n",
    "\n",
    "**Código de función de Lambda**\n",
    "\n",
    "Navegue a la carpeta **Chatbot**, use el menú **Archivo -> Nuevo -> Archivo de Python** para crear un nuevo archivo de Python. Esto crea un archivo de Python sin título en la carpeta **Chatbot**. Cambie el nombre del archivo a **chatbot_v3.py**. Copie y pegue el siguiente contenido en **chatbot_v3.py**. Utilice el menú **Archivo -> Guardar archivo de Python** para guardar el contenido del archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df63451-c52f-4741-8323-365b661edcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import os\n",
    "import requests\n",
    "from requests_aws4auth import AWS4Auth\n",
    "\n",
    "region = os.environ.get('AWS_REGION')\n",
    "bedrock = boto3.client(service_name='bedrock-runtime')\n",
    "aoss_host = os.environ.get('aossHost')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    if (event['httpMethod'] == 'GET'):\n",
    "        output = load_html()\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'headers': {'Content-Type': 'text/html'},\n",
    "            'body': output\n",
    "        }\n",
    "    elif (event['httpMethod'] == 'POST'):\n",
    "        prompt = event['body']\n",
    "        comm, query = check_prompt_command(prompt)\n",
    "        if comm == 'search':\n",
    "            output = search(query, limit=5)\n",
    "        elif comm == 'rag':\n",
    "            # retrieve the context\n",
    "            info = search(query, limit=5)\n",
    "            # augment the prompt with the context\n",
    "            prompt = 'Use the context below to answer the question:\\n\\n=== Context ===\\n{0}\\n\\n=== Question ===\\n{1}'.format(info, query)\n",
    "            output = chat(prompt)\n",
    "        else:\n",
    "            output = chat(prompt)\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'headers': {'Content-Type': 'text/html'},\n",
    "            'body': output\n",
    "        }\n",
    "    else:\n",
    "         return {\n",
    "            'statusCode': 200,\n",
    "            'headers': {'Content-Type': 'text/html'},\n",
    "            'body': \"OK\"\n",
    "        }\n",
    "\n",
    "def check_prompt_command(prompt):\n",
    "    comm = 'chat'\n",
    "    query = None\n",
    "    # Check the last line of the prompt to see if it is a search request.\n",
    "    lines = prompt.splitlines()\n",
    "    last_line = lines[-1]\n",
    "    # Check if the last line starts with \"Human: \"\n",
    "    if last_line.startswith('Human: '):\n",
    "        last_line = last_line[7:].strip()\n",
    "        # Check if the human prompt starts with \"//search \"\n",
    "        if last_line.startswith('//search '):\n",
    "            query = last_line[9:].strip()\n",
    "            if query != None:\n",
    "                comm = 'search'\n",
    "        # Check if the human prompt starts with \"//rag \"\n",
    "        if last_line.startswith('//rag '):\n",
    "            query = last_line[5:].strip()\n",
    "            if query != None:\n",
    "                comm = 'rag'\n",
    "    return comm, query\n",
    "\n",
    "def load_html():\n",
    "    html = ''\n",
    "    with open('index.html', 'r') as file:\n",
    "        html = file.read()\n",
    "    return html\n",
    "\n",
    "def chat(prompt):\n",
    "    modelId = 'ai21.j2-ultra'\n",
    "    accept = 'application/json'\n",
    "    contentType = 'application/json'\n",
    "    body=json.dumps({'prompt': prompt, 'maxTokens': 250, 'stopSequences': ['Human:']})\n",
    "    response = bedrock.invoke_model(body=body, modelId=modelId, accept=accept,contentType=contentType)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    completions = response_body['completions']\n",
    "    output = ''\n",
    "    for part in completions:\n",
    "        output += part['data']['text']\n",
    "    return output\n",
    "\n",
    "def search(query, limit=1):\n",
    "    # get embedding\n",
    "    embedding = get_embedding(query)\n",
    "    # prepare for OpenSearch Serverless\n",
    "    service = 'aoss'\n",
    "    credentials = boto3.Session().get_credentials()\n",
    "    awsauth = AWS4Auth(\n",
    "        credentials.access_key, \n",
    "        credentials.secret_key, \n",
    "        region, \n",
    "        service, \n",
    "        session_token=credentials.token\n",
    "    )\n",
    "    # search\n",
    "    index = 'demo-index'\n",
    "    datatype = '_search'\n",
    "    url = aoss_host + '/' + index + '/' + datatype\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    document = {\n",
    "        'size': limit,\n",
    "        'query': {\n",
    "            'knn': {\n",
    "                'embedding': {\n",
    "                    'vector': embedding,\n",
    "                    'k': limit\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    # response\n",
    "    response = requests.get(url, auth=awsauth, json=document, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    output = ''\n",
    "    for item in data['hits']['hits']:\n",
    "        output += item['_source']['content'] + '\\n'\n",
    "    return output.strip()\n",
    "\n",
    "def get_embedding(text):\n",
    "    modelId = 'amazon.titan-embed-text-v1'\n",
    "    accept = 'application/json'\n",
    "    contentType = 'application/json'\n",
    "    input = {\n",
    "            'inputText': text\n",
    "        }\n",
    "    body=json.dumps(input)\n",
    "    response = bedrock.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept,contentType=contentType)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    embedding = response_body['embedding']\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ba4745-a2ca-4590-9473-244d3f7eda13",
   "metadata": {},
   "source": [
    "La lógica del código es:\n",
    "\n",
    "- Para todas las solicitudes GET, cargamos el contenido HTML de index.html y se lo devolvemos al solicitante.\n",
    "- Para todas las solicitudes POST, extraemos el cuerpo de la solicitud y, a continuación,\n",
    "- Si la última línea de human comienza con , esto se trata como un comando de búsqueda, cualquier cosa después del comando se trata como el texto de la consulta. En este caso, convertimos el texto de la consulta en incrustación y, a continuación, buscamos en la base de conocimiento. Las 5 entradas de datos más cercanas en la base de conocimiento se devuelven al solicitante.//search//search\n",
    "- Si la última línea de human comienza con , esto se trata como una solicitud de RAG. Todo lo que esté después del comando se trata como el texto de la consulta. En este caso, convertimos el texto de la consulta en incrustación, recuperamos las 5 entradas más relevantes de la base de conocimiento, aumentamos la solicitud con el contexto, invocamos el modelo base con la solicitud aumentada y devolvemos la respuesta al solicitante.//rag//rag\n",
    "- De lo contrario, usamos el cuerpo de la solicitud como solicitud para invocar un modelo básico y, a continuación, devolvemos la respuesta del modelo básico al solicitante.\n",
    "- Para todas las demás solicitudes, simplemente devolvemos un OK al solicitante.\n",
    "\n",
    "**Paquete de implementación**\n",
    "\n",
    "En la ventana del terminal, vaya a la carpeta Chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a801dc70-549e-49ba-b2ef-ed860a122282",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/Chatbot\n",
    "\n",
    "#Ejecute los siguientes comandos para volver a compilar el paquete de implementación:\n",
    "rm my_deployment_package.zip\n",
    "cd package\n",
    "zip -r ../my_deployment_package.zip .\n",
    "cd ..\n",
    "zip my_deployment_package.zip index.html\n",
    "zip my_deployment_package.zip chatbot_v1.py\n",
    "zip my_deployment_package.zip chatbot_v2.py\n",
    "zip my_deployment_package.zip chatbot_v3.py\n",
    "\n",
    "#Copie el archivo zip en su bucket de S3.\n",
    "S3_BUCKET=$(aws s3 ls | grep bedrock-workshop | cut -d' ' -f3-)\n",
    "aws s3 cp my_deployment_package.zip s3://$S3_BUCKET\n",
    "\n",
    "#Despliegue\n",
    "#Ahora tenemos que configurar la función de Lambda para utilizar el nuevo paquete de implementación:\n",
    "aws lambda update-function-code --function-name BedrockWorkshopChatbot --s3-bucket $S3_BUCKET --s3-key my_deployment_package.zip\n",
    "\n",
    "#También necesitamos configurar la función Lambda para usar el controlador en chatbot_v3.py.\n",
    "aws lambda update-function-configuration --function-name BedrockWorkshopChatbot --handler chatbot_v3.lambda_handler\n",
    "\n",
    "#Acceda nuevamente a la interfaz de usuario web de su chatbot, pruebe con las siguientes indicaciones una por una para ver cómo funcionan las cosas.\n",
    "#Comienza con un simple saludo:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
