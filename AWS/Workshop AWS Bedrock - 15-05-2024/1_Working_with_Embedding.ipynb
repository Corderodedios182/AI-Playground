{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4356a598-427c-4cd3-8841-10053bd9c1e0",
   "metadata": {},
   "source": [
    "En esta sección, se analizan la incrustación y las aplicaciones relacionadas con la incrustación. \n",
    "Comenzamos con un ejemplo para ilustrar qué es la incrustación, junto con varias formas de calcular la similitud vectorial (distancia euclidiana, similitud del producto del punto, similitud del coseno). \n",
    "Demostramos algunos casos de uso comunes de incrustación, como la búsqueda y recomendación, la clasificación, la agrupación en clústeres y la detección de valores atípicos. \n",
    "Después de eso, presentamos algunas formas de almacenar y buscar datos vectoriales, incluidos archivos planos, pgvector, OpenSearch Serverless y Pinecone.\n",
    "\n",
    "Ahora puede hacer una pregunta y recibe el texto generado.\n",
    "Cuando haces una pregunta de seguimiento, el texto generado parece no estar relacionado con la pregunta y la respuesta anteriores.\n",
    "Cada pregunta parece ser tratada por separado: los modelos de fundación no parecen recordar lo que se ha preguntado y respondido previamente.\n",
    "\n",
    "Al final de esta sección, podrá implementar la búsqueda en lenguaje natural en el servicio de chatbot conversacional sin servidor creado en la sección anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4eb169-2d9e-460d-9d96-0089027ba3af",
   "metadata": {},
   "source": [
    "La definición académica de incrustación es **traducir vectores de alta dimensión en un espacio de baja dimensión**. \n",
    "\n",
    "Es posible que conozcas todas y cada una de las palabras de esta oración, pero aún no tengas idea de toda la oración. Podemos pensar en la incrustación como la conversión del lenguaje natural en una secuencia de números, siendo la entrada un fragmento de texto y la salida un vector. En otras palabras, el vector es una representación numérica del texto, lo que facilita la realización de todo tipo de cálculos complejos en IA/ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76ae58ec-6ec9-4c5e-b764-b20fca9a074d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "bedrock = boto3.client(\n",
    "    service_name='bedrock-runtime'\n",
    ")\n",
    "\n",
    "modelId = 'amazon.titan-embed-text-v1'\n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'\n",
    "prompt = \"\"\"Quiero aprender AWS\"\"\"\n",
    "input = {\n",
    "        'inputText': prompt\n",
    "    }\n",
    "body=json.dumps(input)\n",
    "response = bedrock.invoke_model(\n",
    "    body=body, modelId=modelId, accept=accept,contentType=contentType)\n",
    "response_body = json.loads(response.get('body').read())\n",
    "embedding = response_body['embedding']\n",
    "\n",
    "#El modelo de base decide el número de elementos (dimensiones) en el vector, no el texto de entrada. Con el modelo amazon.titan-embed-text-v1, el tamaño del vector de salida es 1536.\n",
    "print(len(embedding))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f21c1bc-b3a9-426b-88dd-f2ffaf602489",
   "metadata": {},
   "source": [
    "**Distancia euclidiana**\n",
    "\n",
    "Cuando se utilizan vectores para representar texto, se puede calcular la distancia euclidiana entre dos fragmentos de texto.\n",
    "\n",
    "Aquí está el código de ejemplo para calcular la distancia euclidiana entre \"hola\" y \"buen día\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d2904d7-c24a-4473-9978-1f7494d551b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.43179560150841\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "import math\n",
    "\n",
    "def get_embedding(bedrock, text):\n",
    "    modelId = 'amazon.titan-embed-text-v1'\n",
    "    accept = 'application/json'\n",
    "    contentType = 'application/json'\n",
    "    input = {\n",
    "            'inputText': text\n",
    "        }\n",
    "    body=json.dumps(input)\n",
    "    response = bedrock.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept,contentType=contentType)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    embedding = response_body['embedding']\n",
    "    return embedding\n",
    "\n",
    "def calculate_distance(v1, v2):\n",
    "    distance = math.dist(v1, v2)\n",
    "    return distance\n",
    "\n",
    "# main function\n",
    "bedrock = boto3.client(\n",
    "    service_name='bedrock-runtime'\n",
    ")\n",
    "\n",
    "text1 = 'AI'\n",
    "text2 = 'ML'\n",
    "v1 = get_embedding(bedrock, text1)\n",
    "v2 = get_embedding(bedrock, text2)\n",
    "distance = calculate_distance(v1, v2)\n",
    "print(distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a60f8e-60cb-4bbf-b0e8-e0a344bcb59d",
   "metadata": {},
   "source": [
    "Quizás te preguntes por qué a la gente le importa \"la distancia entre dos piezas de texto\". ¿Qué significa realmente?\n",
    "\n",
    "La distancia euclidiana entre dos fragmentos de texto indica la distancia a la que se encuentran entre sí. En términos legibles por humanos, significa qué tan similares son en el lenguaje natural. Si la distancia es muy pequeña, los dos fragmentos de texto transmiten un mensaje similar. Si la distancia es muy grande, los dos fragmentos de texto transmiten un mensaje diferente.\n",
    "\n",
    "Reutilizando el código mencionado anteriormente, calculamos la distancia euclidiana entre \"hola\" y un montón de textos diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "99812c15-c97f-4a10-aaff-6a9bdf17c002",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_comparative = \"Xochimilco\"\n",
    "\n",
    "texts = [\n",
    "    'lago',\n",
    "    'trajinearas',\n",
    "    'caidas',\n",
    "    'mariachi',\n",
    "    'palomas',\n",
    "    \"tequilas\",\n",
    "    'policia',\n",
    "    'laptop'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "86b44e06-0062-4e16-baca-b8e02112f90b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>distance_euclidian</th>\n",
       "      <th>Rank_euclidian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lago</td>\n",
       "      <td>27.620055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trajinearas</td>\n",
       "      <td>26.271369</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>caidas</td>\n",
       "      <td>26.468978</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mariachi</td>\n",
       "      <td>25.565576</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>palomas</td>\n",
       "      <td>24.265096</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tequilas</td>\n",
       "      <td>22.818800</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>policia</td>\n",
       "      <td>27.307918</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>laptop</td>\n",
       "      <td>33.889377</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         words  distance_euclidian  Rank_euclidian\n",
       "0         lago           27.620055               7\n",
       "1  trajinearas           26.271369               4\n",
       "2       caidas           26.468978               5\n",
       "3     mariachi           25.565576               3\n",
       "4      palomas           24.265096               2\n",
       "5     tequilas           22.818800               1\n",
       "6      policia           27.307918               6\n",
       "7       laptop           33.889377               8"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main function\n",
    "bedrock = boto3.client(\n",
    "    service_name='bedrock-runtime'\n",
    ")\n",
    "hello = get_embedding(bedrock, word_comparative)\n",
    "\n",
    "texts_distance = []\n",
    "for text in texts:\n",
    "    embedding = get_embedding(bedrock, text)\n",
    "    distance = calculate_distance(hello, embedding)\n",
    "    texts_distance.append(distance)\n",
    "\n",
    "distance_euclidian = pd.DataFrame(columns=[texts,texts_distance]).transpose().reset_index()\n",
    "distance_euclidian.columns = [\"words\",\"distance_euclidian\"]\n",
    "\n",
    "distance_euclidian['Rank_euclidian'] = distance_euclidian.sort_values(by=['distance_euclidian'], ascending=True) \\\n",
    "                                                         .reset_index() \\\n",
    "                                                         .sort_values('index') \\\n",
    "                                                         .index + 1\n",
    "\n",
    "distance_euclidian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52dfe3d-0f91-4f1d-896a-14989d970423",
   "metadata": {},
   "source": [
    "**Similitud del producto Dot**\n",
    "\n",
    "La distancia euclidiana no es la única forma de medir la similitud entre dos vectores. La similitud de productos escalar es otro método comúnmente utilizado para medir la similitud. En álgebra lineal, el producto escalar entre dos vectores mide el grado en que dos vectores se alinean en la misma dirección. Si el producto escalar de dos vectores es 0, los dos vectores son ortogonales (perpendiculares), que es una especie de similitud intermedia.\n",
    "\n",
    "Aquí está el código de ejemplo para calcular la similitud del producto punto entre \"hola\" y \"buen día\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ebea04d0-3e06-48c3-90dd-92b5f46ef590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245.3465466016232\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def get_embedding(bedrock, text):\n",
    "    modelId = 'amazon.titan-embed-text-v1'\n",
    "    accept = 'application/json'\n",
    "    contentType = 'application/json'\n",
    "    input = {\n",
    "            'inputText': text\n",
    "        }\n",
    "    body=json.dumps(input)\n",
    "    response = bedrock.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept,contentType=contentType)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    embedding = response_body['embedding']\n",
    "    return embedding\n",
    "\n",
    "def calculate_dot_product_similarity(v1, v2):\n",
    "    similarity = dot(v1, v2)\n",
    "    return similarity\n",
    "\n",
    "# main function\n",
    "bedrock = boto3.client(\n",
    "    service_name='bedrock-runtime'\n",
    ")\n",
    "text1 = 'hello'\n",
    "text2 = 'good day'\n",
    "v1 = get_embedding(bedrock, text1)\n",
    "v2 = get_embedding(bedrock, text2)\n",
    "similarity = calculate_dot_product_similarity(v1, v2)\n",
    "print(similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304cf5b7-130e-49f5-a423-146a4f4b3e70",
   "metadata": {},
   "source": [
    "Si el ángulo entre dos vectores es menor que 90 grados, los dos vectores están aproximadamente en la misma dirección, entonces el producto escalar es positivo. Si el ángulo entre dos vectores es mayor que 90 grados, los dos vectores están aproximadamente en direcciones opuestas, entonces el producto escalar es negativo. Cuanto más grande sea el producto escalonado, es más probable que los dos vectores estén alineados en la misma dirección.\n",
    "\n",
    "Reutilizando el código mencionado anteriormente, calculamos la similitud del producto punto entre \"hola\" y un montón de textos diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c6d458bb-12ce-43ef-acc0-cfe42aa316d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>similarity_dot_product</th>\n",
       "      <th>Rank_dot_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lago</td>\n",
       "      <td>154.109042</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trajinearas</td>\n",
       "      <td>115.263363</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>caidas</td>\n",
       "      <td>165.162233</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mariachi</td>\n",
       "      <td>175.569933</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>palomas</td>\n",
       "      <td>144.825127</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tequilas</td>\n",
       "      <td>240.368924</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>policia</td>\n",
       "      <td>139.765334</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>laptop</td>\n",
       "      <td>46.699242</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         words  similarity_dot_product  Rank_dot_product\n",
       "0         lago              154.109042                 5\n",
       "1  trajinearas              115.263363                 2\n",
       "2       caidas              165.162233                 6\n",
       "3     mariachi              175.569933                 7\n",
       "4      palomas              144.825127                 4\n",
       "5     tequilas              240.368924                 8\n",
       "6      policia              139.765334                 3\n",
       "7       laptop               46.699242                 1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main function\n",
    "bedrock = boto3.client(\n",
    "    service_name='bedrock-runtime'\n",
    ")\n",
    "hello = get_embedding(bedrock, word_comparative)\n",
    "\n",
    "texts_similarity = []\n",
    "for text in texts:\n",
    "    embedding = get_embedding(bedrock, text)\n",
    "    similarity = calculate_dot_product_similarity(hello, embedding)\n",
    "    texts_similarity.append(similarity)\n",
    "\n",
    "distance_dot_product = pd.DataFrame(columns=[texts,texts_similarity]).transpose().reset_index()\n",
    "distance_dot_product.columns = [\"words\",\"similarity_dot_product\"]\n",
    "\n",
    "distance_dot_product['Rank_dot_product'] = distance_dot_product.sort_values(by=['similarity_dot_product'], ascending=True) \\\n",
    "                                                             .reset_index() \\\n",
    "                                                             .sort_values('index') \\\n",
    "                                                             .index + 1\n",
    "\n",
    "distance_dot_product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb68a51-9a79-4d0b-8d61-1e39d32bf27d",
   "metadata": {},
   "source": [
    "**Similitud de coseno**\n",
    "\n",
    "La similitud del coseno es otro método comúnmente utilizado para medir la similitud. En álgebra lineal, la similitud del coseno es el coseno del ángulo entre dos vectores. Es decir, es el producto escalar de los vectores dividido por el producto de sus longitudes.\n",
    "\n",
    "Aquí está el código de ejemplo para calcular la similitud del coseno entre \"hola\" y \"buen día\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "05232161-8f6f-4d74-b1dc-1ce650db481e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47547669109839213\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def get_embedding(bedrock, text):\n",
    "    modelId = 'amazon.titan-embed-text-v1'\n",
    "    accept = 'application/json'\n",
    "    contentType = 'application/json'\n",
    "    input = {\n",
    "            'inputText': text\n",
    "        }\n",
    "    body=json.dumps(input)\n",
    "    response = bedrock.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept,contentType=contentType)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    embedding = response_body['embedding']\n",
    "    return embedding\n",
    "\n",
    "def calculate_cousin_similarity(v1, v2):\n",
    "    similarity = dot(v1, v2)/(norm(v1)*norm(v2))\n",
    "    return similarity\n",
    "\n",
    "# main function\n",
    "bedrock = boto3.client(\n",
    "    service_name='bedrock-runtime'\n",
    ")\n",
    "text1 = 'hello'\n",
    "text2 = 'good day'\n",
    "v1 = get_embedding(bedrock, text1)\n",
    "v2 = get_embedding(bedrock, text2)\n",
    "similarity = calculate_cousin_similarity(v1, v2)\n",
    "print(similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7898d3be-caed-40ff-bd88-a5ab2bfd7cd0",
   "metadata": {},
   "source": [
    "Al igual que la distancia euclidiana, la similitud del coseno mide la similitud de dos textos en cuanto a su tema, que es independiente de la longitud de los textos. Cuando la similitud del coseno es igual a 1, significa que dos fragmentos de texto tienen significados idénticos en el lenguaje natural.\n",
    "\n",
    "Reutilizando el código mencionado anteriormente, calculamos la similitud del coseno entre \"hola\" y un montón de textos diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fe675bd7-9576-41a3-8b2a-699827a6068f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>similarity_cousins_similarity</th>\n",
       "      <th>Rank_cousins_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lago</td>\n",
       "      <td>0.289204</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trajinearas</td>\n",
       "      <td>0.250660</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>caidas</td>\n",
       "      <td>0.321085</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mariachi</td>\n",
       "      <td>0.349767</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>palomas</td>\n",
       "      <td>0.331316</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tequilas</td>\n",
       "      <td>0.480379</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>policia</td>\n",
       "      <td>0.273129</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>laptop</td>\n",
       "      <td>0.077159</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         words  similarity_cousins_similarity  Rank_cousins_similarity\n",
       "0         lago                       0.289204                        4\n",
       "1  trajinearas                       0.250660                        2\n",
       "2       caidas                       0.321085                        5\n",
       "3     mariachi                       0.349767                        7\n",
       "4      palomas                       0.331316                        6\n",
       "5     tequilas                       0.480379                        8\n",
       "6      policia                       0.273129                        3\n",
       "7       laptop                       0.077159                        1"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main function\n",
    "bedrock = boto3.client(\n",
    "    service_name='bedrock-runtime'\n",
    ")\n",
    "hello = get_embedding(bedrock, word_comparative)\n",
    "\n",
    "text_cousins_similarity = []\n",
    "for text in texts:\n",
    "    embedding = get_embedding(bedrock, text)\n",
    "    similarity = calculate_cousin_similarity(hello, embedding)\n",
    "    text_cousins_similarity.append(similarity)\n",
    "\n",
    "distance_cousin_similarity = pd.DataFrame(columns=[texts,text_cousins_similarity]).transpose().reset_index()\n",
    "distance_cousin_similarity.columns = [\"words\",\"similarity_cousins_similarity\"]\n",
    "\n",
    "distance_cousin_similarity['Rank_cousins_similarity'] = distance_cousin_similarity.sort_values(by=['similarity_cousins_similarity'], ascending=True) \\\n",
    "                                                                      .reset_index() \\\n",
    "                                                                      .sort_values('index') \\\n",
    "                                                                      .index + 1\n",
    "\n",
    "distance_cousin_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0299fe8-ba7d-4532-9e16-b3d3ebc21e5d",
   "metadata": {},
   "source": [
    "**Discusiones**\n",
    "\n",
    "En la tabla siguiente se comparan los resultados producidos por la distancia euclidiana, la similitud del producto escalar y la similitud del coseno. Los números de la tabla representan la posición de cada palabra, ordenada por la similitud entre la palabra y \"hola\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ef6fefde-8d3d-455c-9d87-2878c33531d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xochimilco\n",
      "Diferentes similitudes :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>Rank_euclidian</th>\n",
       "      <th>Rank_dot_product</th>\n",
       "      <th>Rank_cousins_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lago</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trajinearas</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>caidas</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mariachi</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>palomas</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tequilas</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>policia</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>laptop</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         words  Rank_euclidian  Rank_dot_product  Rank_cousins_similarity\n",
       "0         lago               7                 5                        4\n",
       "1  trajinearas               4                 2                        2\n",
       "2       caidas               5                 6                        5\n",
       "3     mariachi               3                 7                        7\n",
       "4      palomas               2                 4                        6\n",
       "5     tequilas               1                 8                        8\n",
       "6      policia               6                 3                        3\n",
       "7       laptop               8                 1                        1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(word_comparative)\n",
    "print(\"Diferentes similitudes :\")\n",
    "pd.concat([distance_euclidian,distance_dot_product,distance_cousin_similarity], axis = 1).iloc[:,[0,2,5,8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532bb353-8c5e-40e6-bf34-219d057b12c2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
